---
title: "Small-Example"
author: "David Ory"
html_document:
    toc: true
    theme: cosmo
runtime: shiny
---

## Administration

#### Status
seems to be working

#### Purpose
Demonstrate multi-criteria transit on-board survey expansion using R tools via a small (i.e., non-trivial, but short of production) example using real data.  Here, we use two surveys performed on Tri-Delta.  The first survey was a simple on-off count performed for ~70 percent of the riders; we assume this data represents the population ridership characteristics.  The second survey is a personal interview survey containing typical questions; this survey has already been expanded to external data, which will give us some results to compare to.

#### Overhead
```{r}
library(knitr)
library(stringr)
library(optimx)
library(reshape2)
suppressMessages(library(dplyr))
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

## Data Preparation
#### Remote data locations
```{r data-reads}
input_dir  <- "M:/Data/OnBoard/Data and Reports/Tri Delta/As CSV/"
output_dir <- "D:/files/My Box Files/Share Data/multi-criteria-expansion/"

input.tablet <- read.table(file = paste(input_dir,"ONBOARD NO POUND OR SINGLE QUOTE.csv", sep = ""), header = TRUE, sep = ",", stringsAsFactors = FALSE)

input.onoff  <- read.table(file = paste(input_dir, "ON2OFF NO POUND OR SINGLE QUOTE.csv", sep = ""), header = TRUE, sep = ",", stringsAsFactors = FALSE)

```

#### Data preparation
```{r prepare-datasets}
# Survey data, align variables
survey <- input.tablet %>%
  select(tablet_id = ID, 
         route_dir = ROUTE.CODE, 
         direction = DIRECTION, 
         time_period = TIME_PERIOD, 
         weight = UNLINKED_WGHT_FCTR) %>%
  mutate(route = as.numeric(str_sub(route_dir, 1, 3))) %>%
  filter(route != 392) %>%
  filter(route != 393) %>%
  filter(route != 394) %>%
  filter(route != 395) %>%
  mutate(time_period = ifelse(time_period == "LATE PM", "EVENING", time_period))

# Use on/off survey as a proxy for detailed observed data
observed <- input.onoff %>%
  select(onoff_id = ETC_ID, 
         route = ROUTE_ON, 
         direction = DIRECTION_ON, 
         time_period = TIME.PERIOD, 
         MATCH_CONFIDENCE = matchConfidence) %>%
  mutate(time_period = ifelse(time_period == "AM1", "EARLY AM", time_period)) %>%
  mutate(time_period = ifelse(time_period == "AM2", "AM PEAK",  time_period)) %>%
  mutate(time_period = ifelse(time_period == "MID", "MIDDAY",   time_period)) %>%
  mutate(time_period = ifelse(time_period == "PM1", "PM PEAK",  time_period)) %>%
  mutate(time_period = ifelse(time_period == "PM2", "EVENING",  time_period))
````


## Optimization Function
#### Inputs
1. `x` - a vector of length M representing each of the unique survey weights that need to be calculated
2. `obs_target_v` - a vector of length N containing each expansion target
3. `import_v` - a vector of length N containing the importance weight for each expansion target
4. `inc_mtx` - a matrix of dimensions M x N containing a dummy variable denoting the relevance of each unique survey weight to each expansion target
```{r optimization-function}
optimization_function <- function(x, obs_target_v, import_v, inc_mtx) {
  
  # Compute estimated targets 
  est_target_v <- x %*% inc_mtx
  
  # Compute importance-weighted errors, which is the objective function
  error_df <- data.frame(obs_target_v, est_target_v, import_v)
  error_df <- error_df %>%
    mutate(error <- import_v * abs(est_target_v - obs_target_v))
  
  # Return errors
  return(sum(error_df$error))

}

```

## Small-Example
Expand the Tri-Delta survey by route, direction, and time-of-day.  Also consider a system-wide ridership target to act some interaction in the optimization.

#### Prepare the observed targets vector
```{r observed-targets}
observed_targets <- observed %>%
  filter(MATCH_CONFIDENCE == "100%") %>%
  group_by(route, direction, time_period) %>%
  summarise(target_count = n()) %>%
  ungroup() %>%
  mutate(target_id = paste(route, "---", direction, "---", time_period, sep = "")) %>%
  select(target_id, target_count)

# Add the all routes target, scale up to match the sum of the existing survey weights (9849)
all_route_target <- data.frame(target_id = c("all_routes"), target_count = c(9849))

observed_targets <- rbind(observed_targets, all_route_target)

observed_targets_vector <- observed_targets$target_count

```

#### Build the importance weight vector
```{r importance-weights}
# Route, direction, time-of-day weights are assumed to be 1.0; system target (last element) is more important with a weight of 10.0
importance_weights_vector <- observed_targets_vector * 0 + 1.0
importance_weights_vector[length(observed_targets_vector)] <- 10.0

```

#### Build the incidence matrix
```{r incidence-matrix}
# The on/off survey did not capture routes 392, 393, 394, or 395 -- remove from survey
unique_weights <- survey %>%
  group_by(route, direction, time_period) %>%
  summarise(records = n()) %>%
  mutate(target_id = paste(route, "---", direction, "---", time_period, sep = ""))

# Create a data frame with the targets rows and columns, with the diagonal set at 1, and all_routes column set to 1
observed_targets_incidence <- observed_targets %>%
  mutate(one = 1)
observed_targets_incidence <- dcast(observed_targets_incidence, target_id ~ target_id, value.var = "one")
observed_targets_incidence[is.na(observed_targets_incidence)] <- 0
observed_targets_incidence <- observed_targets_incidence %>%
  mutate(all_routes = 1)

# Join the unique weights targets with the observed incidence dummies
incidence_matrix <- unique_weights %>%
  ungroup() %>%
  select(target_id)

incidence_matrix <- left_join(incidence_matrix, observed_targets_incidence, by = c("target_id"))

# Put the results in a unique weight count by number of targets matrix
incidence_matrix <- incidence_matrix %>%
  select(-target_id)
incidence_matrix <- data.matrix(incidence_matrix)
  
```

#### Execute the optimization and extract results
```{r run-optimization}
# Create a vector of starting weights
starting_weights_vector <- unique_weights$records * 0 + 1.0

# Run the optimization
optimx_results <- optimx(starting_weights_vector,
                         fn = optimization_function,
                         method = "L-BFGS-B",
                         lower = 1.00,
                         upper = Inf,
                         obs_target_v = observed_targets_vector,
                         import_v = importance_weights_vector,
                         inc_mtx = incidence_matrix)

sum_unique_weights <- as.data.frame(t(coef(optimx_results)))
names(sum_unique_weights)[1] <- "sum_weights"
sum_unique_weights <- cbind(sum_unique_weights, target_id = unique_weights$target_id)

```

#### Join optimization results to survey data
```{r join-results}
survey_summary <- left_join(unique_weights, sum_unique_weights, by = c("target_id"))

survey_summary <- survey_summary %>%
  mutate(record_weight = sum_weights / records) %>%
  select(-target_id, -sum_weights, -records)

survey <- left_join(survey, survey_summary, by = c("route", "direction", "time_period"))

```

#### Explore the difference in the two sets of weights
```{r weight-diff}
survey_explore <- survey %>%
  mutate(abs_diff = abs(record_weight - weight)) %>%
  mutate(diff = record_weight - weight) %>%
  arrange(desc(abs_diff))

table(survey_explore$diff)

```

## Data writes
#### Write the example extracted input files to disk for others' reference
```{r data-writes}
write.csv(survey,   file = paste(output_dir, "small-example-survey.csv", sep = ""),   row.names = FALSE, quote = F)
write.csv(observed, file = paste(output_dir, "small-example-observed.csv", sep = ""), row.names = FALSE, quote = F)

```

