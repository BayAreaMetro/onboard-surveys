---
title: "Production-Example"
author: "David Ory"
html_document:
    toc: true
    theme: cosmo
runtime: shiny
---

## Administration

#### Status
Seems to be working, need to refine, improve, refine

#### TODO
1. Can we make the optimization faster?  Other methods?

2. Experiment with changing the target importance weights to make sure the response is as expected

3. Can we put common elements into an R Package? Or perhaps simply an R script with a library of methods?

#### Purpose
Demonstrate multi-criteria transit on-board survey expansion using R tools via a production (i.e.,  practice-ready) example using real data.  Here, we use two surveys performed on Tri-Delta.  The first survey was a simple on-off count performed for ~70 percent of the riders.  The second is a personal interview survey of ~5 percent of the riders. Here we use external files to define our targets.  Please see `build-production-example-inputs.Rmd` for the creation of these files.  Narrative details are as follows:

1.  Five categories of targets are defined, as follows:
(a)  Category A: single, system-wide target;
(b)  Category B: by route, direction, and time of day;
(c)  Category C: by route, direction, and four-category boarding segment;
(d)  Category D: by route, direction, and four-category alighting segment;
(e)  Category E: by route, direction, three-category boarding segment, and three-category alighting segment.

Note that if a single, system-wide target is used, it must be in Category A (i.e., be listed first).

2.  Target categories A and B are drawn for the personal interview survey; categories C, D, and E are drawn from the on-off survey.

3.  The `observed-target-counts` input database must have the following fields:
(a) `target_id` - integer, a unique integer for the target;
(b) `target_count` - float, the target value for the target (i.e., the weights will be adjusted to match the targets);
(c) `importance_weight` - float, the importance weight for the target;
(d) `target_category` - string, a label for each group of targets;
(e) {survey variable names} - any number of columns that define the targets composition using the survey variable names, e.g., route, direction, time_period.  Each string MUST correspond to a variable in the survey data. 

4.  The `observed-target-definitions` input database must have the following fields:
(a) `target_category` - string, a label for each group of targets;
(b) `survey_variable` - string, a label for the variables that comprise the target, with each variable being entered on a separate row in the database.  Each string MUST correspond to a variable in the survey data, with the exception of `all_routes` which denotes a target that is applied to all records in the survey.


#### Overhead
```{r}
library(knitr)
library(stringr)
library(optimx)
library(reshape2)
suppressMessages(library(dplyr))
library(ggplot2)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

#### Parameters
```{r parameters}
all_routes_string = "all_routes"
record_weight_lower_bound = 0.5
record_weight_upper_bound = 50.0

```


## Optimization Function
#### Inputs
1. `x` - a vector of length M representing each of the unique survey weights that need to be calculated
2. `obs_target_v` - a vector of length N containing each expansion target
3. `import_v` - a vector of length N containing the importance weight for each expansion target
4. `inc_mtx` - a matrix of dimensions M x N containing a dummy variable denoting the relevance of each unique survey weight to each expansion target
```{r optimization-function}
optimization_function <- function(x, obs_target_v, import_v, inc_mtx) {
  
  # Compute estimated targets 
  est_target_v <- x %*% inc_mtx
  
  # Compute importance-weighted errors, which is the objective function
  error_df <- data.frame(obs_target_v, est_target_v, import_v)
  error_df <- error_df %>%
    mutate(error <- import_v * abs(est_target_v - obs_target_v))
  
  # Return errors
  return(sum(error_df$error))

}

```

## Data Preparation
#### Remote data locations
```{r data-reads}
remote_dir <- "D:/files/My Box Files/Share Data/multi-criteria-expansion/"

F_INPUT_SURVEY  <- paste(remote_dir, "production-example-survey.Rdata", sep = "")
F_INPUT_DEFNS   <- paste(remote_dir, "observed-target-counts.Rdata", sep = "")
F_INPUT_TARGETS <- paste(remote_dir, "observed-target-definitions.Rdata", sep = "")

load(F_INPUT_SURVEY)
load(F_INPUT_DEFNS)
load(F_INPUT_TARGETS)
```

## Optimization Input Perparation
#### Prepare the observed targets vector
```{r observed-targets}
observed_targets_vector <- observed_targets$target_count
```

#### Prepare the importance weights vector
```{r importance-weights}
importance_weights_vector <- observed_targets$importance_weight
```

#### Prepare the incidence matrix
```{r incidence-matrix}
# TODO: add some checking, debugging

# Make sure the defined survey_variables are strings
observed_targets_defn <- observed_targets_defn %>%
  mutate(survey_variable = as.character(survey_variable))
  
# Extract a vector of the unique variables in the targets
unique_variables <- data.frame(survey_variable = unique(observed_targets_defn$survey_variable))

# Remove special case "all_routes"
unique_variables <- unique_variables %>%
  filter(survey_variable != all_routes_string) %>%
  mutate(survey_variable = as.character(survey_variable))

# Condensed the survey to the set of unique weights needed
unique_weights <- survey %>%
  group_by_(.dots = unique_variables$survey_variable) %>%
  summarise(records = n()) %>%
  ungroup()

# Add the special case all_routes incidence column
all_routes_column <- observed_targets_defn %>%
  filter(survey_variable == all_routes_string)

all_routes_column <- left_join(all_routes_column, observed_targets, by = c("target_category_id"))

all_routes_column_name <- all_routes_column$target_id

incidence_matrix <- unique_weights %>%
  mutate(one = 1)

names(incidence_matrix)[names(incidence_matrix) == "one"] <- all_routes_column_name 

# To create the rest of the incidence matrix, add a column of ones to the observed_targets dataframe
observed_targets <- observed_targets %>%
  mutate(one = 1)

# Get the vector of target_categories
target_categories_vector <- unique(observed_targets_defn$target_category_id)

# Loop over the target categories
for (i in 1:length(target_categories_vector)) {
  
  # Loop over the variable names in the target category
  variable_names <- observed_targets_defn %>%
    filter(target_category_id == target_categories_vector[i])
  
  variable_names <- variable_names$survey_variable
  
  # Account for all_routes exception
  if (length(variable_names) == 1) next
  
  # Build the formula string that we'll use in the casting
  formula_string <- ""
  for (j in 1:length(variable_names)) {

    if (j > 1) formula_string <- paste(formula_string, "+", sep = " ")
    
    formula_string <- paste(formula_string, variable_names[j], sep = " ")
    
  } # end for j
  
  formula_string <- paste(formula_string, "~ target_id", sep = " ")
  
  # select the relevant columns to cast
  these_targets <- observed_targets %>%
    filter(target_category_id == target_categories_vector[i])
  
  casted <- dcast(these_targets, formula_string, value.var = "one")
  casted[is.na(casted)] <- 0
  
  incidence_matrix <- left_join(incidence_matrix, casted, by = variable_names)
  
} # end for i

# Trim the survey variable columns
unique_variables_v <- unique_variables$survey_variable
for (i in 1:length(unique_variables_v)) {
  incidence_matrix <- incidence_matrix %>%
    select(-matches(unique_variables_v[i]))
}

# Trim the records column
incidence_matrix <- incidence_matrix %>%
  select(-records)

# Fill the NAs with 0
incidence_matrix[is.na(incidence_matrix)] <- 0

# Save as matrix
incidence_matrix <- data.matrix(incidence_matrix)

```

#### Execute the optimization and extract the results
```{r run-optimization}
# Set the lower and upper bounds for the weights (set in parameters section)
unique_weights <- unique_weights %>%
  mutate(minimum_weights = records * record_weight_lower_bound) %>%
  mutate(maximum_weights = records * record_weight_upper_bound)

# Set the starting weights as the minimum weights (note starting_weigths >= minimum_weights)
starting_weights_vector <- unique_weights$minimum_weights

# Test the optimization function
optimization_function(starting_weights_vector, observed_targets_vector, importance_weights_vector, incidence_matrix)
  
# Run the optimization (keep track of computing time)
start_time <- proc.time()
optimx_results <- optimx(starting_weights_vector,
                         fn = optimization_function,
                         method = "L-BFGS-B",
                         lower = unique_weights$minimum_weights,
                         upper = unique_weights$maximum_weights,
                         obs_target_v = observed_targets_vector,
                         import_v = importance_weights_vector,
                         inc_mtx = incidence_matrix)
proc.time() - start_time
# ~2400 to 2900 seconds (~40 to 50 minutes)


sum_unique_weights <- as.data.frame(t(coef(optimx_results)))
names(sum_unique_weights)[1] <- "sum_weights"
```

#### Join optimization results to survey data
```{r results-to-data}
unique_weights <- unique_weights 
survey_summary <- cbind(unique_weights, sum_unique_weights)

survey_summary <- survey_summary %>%
  mutate(record_weight = sum_weights / records) %>%
  select(-sum_weights, -records)

survey <- left_join(survey, survey_summary, by = unique_variables$survey_variable)

```

## Assess Performance
#### Define root-mean-square-error method
```{r rmse-function}
compute_target_rmse <- function(obs_targets_df, target_defn_df, survey_df, category_string){
  
  target <- obs_targets_df %>%
    filter(target_category_id == category_string)
  
  relevant_variables <- target_defn_df %>%
    filter(target_category_id == category_string)
  
  target_compare <- survey_df %>%
    group_by_(.dots = relevant_variables$survey_variable) %>%
    summarise(sum_base_weights = sum(weight), sum_optim_weights = sum(record_weight))
  
  target_compare <- left_join(target_compare, target, by = relevant_variables$survey_variable)
  
  target_compare <- target_compare %>%
    filter(!is.na(target_count)) %>%
    mutate(base_error  = sum_base_weights - target_count) %>%
    mutate(optim_error = sum_optim_weights - target_count) %>%
    mutate(base_error_sqd = base_error * base_error) %>%
    mutate(optim_error_sqd = optim_error * optim_error)
  
  return(c(sqrt(mean(target_compare$base_error_sqd)),
           sqrt(mean(target_compare$optim_error_sqd))))
    
}

```

#### Explore the difference in the two sets of weights
```{r compare-weights}
survey_explore <- survey %>%
  mutate(diff = record_weight - weight) %>%
  mutate(abs_diff = abs(diff)) %>%
  arrange(desc(abs_diff))

# Compare the two sets of weights against each of the targets

# Target A: system-wide
target <- observed_targets %>%
  filter(target_category_id == "A")
abs(sum(survey$weight) - sum(target$target_count))
abs(sum(survey$record_weight) - sum(target$target_count))

# Base: 0; Optim: 0

# Target B: route, direction, time of day
rmse_vector <- compute_target_rmse(observed_targets, observed_targets_defn, survey, "B")
rmse_vector[1]
rmse_vector[2]

# Base: 0; Optim: 63.5

# Target C: route, direction, four-category boarding segment
rmse_vector <- compute_target_rmse(observed_targets, observed_targets_defn, survey, "C")
rmse_vector[1]
rmse_vector[2]

# Base: 154.6; Optim: 45.7

# Target D: route, direction, four-category alighting segment
rmse_vector <- compute_target_rmse(observed_targets, observed_targets_defn, survey, "D")
rmse_vector[1]
rmse_vector[2]

# Base: 139.6; Optim: 58.7

# Target E: route, direction, three-category boarding segment, three-category alighting segment
rmse_vector <- compute_target_rmse(observed_targets, observed_targets_defn, survey, "E")
rmse_vector[1]
rmse_vector[2]

# Base: 98.9; Optim: 20.2
```

#### Plot the distributions
```{r plot-distributions}
weight_data <- survey %>%
  select(weight) %>%
  mutate(type = "original")

record_weight_data <- survey %>%
  select(weight = record_weight) %>%
  mutate(type = "optimal")

weight_data <- rbind(weight_data, record_weight_data)

ggplot(weight_data, aes (x = weight, fill = type, binwidth = 1.0)) + geom_density()

```


## Data writes
#### Write the example results to disck for others' reference
```{r data-writes}
write.csv(survey, file = paste(remote_dir, "production-example-survey-results.csv",   sep = ""), row.names = FALSE, quote = F)

```

